{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from konlpy.tag import Mecab\n",
    "import os\n",
    "import collections\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "\n",
    "mecab = Mecab() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_han = pd.read_csv('./my_data/han/train_data.csv', index_col=0)\n",
    "val_han = pd.read_csv('./my_data/han/valid_data.csv', index_col=0)\n",
    "test_han = pd.read_csv('./my_data/han/test_data.csv', index_col=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_han.cleaned_data = train_han.cleaned_data.apply(eval)\n",
    "val_han.cleaned_data = val_han.cleaned_data.apply(eval)\n",
    "test_han.cleaned_data = test_han.cleaned_data.apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_han['for tokenizer'] = train_han['for tokenizer'].apply(eval)\n",
    "val_han['for tokenizer'] = val_han['for tokenizer'].apply(eval)\n",
    "test_han['for tokenizer'] = test_han['for tokenizer'].apply(eval) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lda = pd.read_csv('./my_data/lda/train_data.csv', index_col=0)\n",
    "val_lda = pd.read_csv('./my_data/lda/valid_data.csv', index_col=0)\n",
    "test_lda = pd.read_csv('./my_data/lda/test_data.csv', index_col=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lda.cleaned_data = train_lda.cleaned_data.apply(eval)\n",
    "val_lda.cleaned_data = val_lda.cleaned_data.apply(eval)\n",
    "test_lda.cleaned_data = test_lda.cleaned_data.apply(eval)\n",
    "train_lda['for tokenizer'] = train_lda['for tokenizer'].apply(eval)\n",
    "val_lda['for tokenizer'] = val_lda['for tokenizer'].apply(eval)\n",
    "test_lda['for tokenizer'] = test_lda['for tokenizer'].apply(eval) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import re \n",
    "import sys\n",
    "import os \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten, Activation \n",
    "from keras.layers import Conv1D, MaxPooling1D, merge, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Model \n",
    "\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers as initializers, regularizers, constraints\n",
    "\n",
    "max_sent_length= 100 \n",
    "max_sentences = 30 \n",
    "max_words_dic = 20000\n",
    "embedding_dim = 100 \n",
    "validation_split = 0.2 \n",
    "adam = optimizers.Adam(lr=0.001) \n",
    "rmsprop = optimizers.RMSprop(lr=0.001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>for tokenizer</th>\n",
       "      <th>전체</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>73688</th>\n",
       "      <td>[유전, 자료, 기반, 혈통, 재구, 보정, 기법, 확립, 고밀, 칩, 기반, 유전...</td>\n",
       "      <td>유전형 자료 기반 혈통 재구성 및 보정기법 확립\\n\\n고밀도칩 기반 유전체 혈연행렬...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52162</th>\n",
       "      <td>[회전자, 내경, 가공, 장치, 개발, 정확, 위치, 프레임, 부품, 고정, 개발,...</td>\n",
       "      <td>？ 회전자의 내경 가공 장치 개발\\n\\n   - 정확한 위치에 프레임과 부품을 고정...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159853</th>\n",
       "      <td>[연구, 공격, 취약점, 악용, 장치, 오작동, 탐지, 가상, 물리, 시스템, 내재...</td>\n",
       "      <td>본 연구에서는 알려진 또는 알려지지 않은 공격을 통해 취약점을 악용하는 IoT 장치...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113271</th>\n",
       "      <td>[주인, 집, 경우, 집, 안, 반려, 견, 스트레스, 정서, 안정, 예방, 외부,...</td>\n",
       "      <td>- 주인이 집을 비울 경우 집 안에 홀로 남겨진 반려견의 스트레스와 정서적 불안정 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129806</th>\n",
       "      <td>[미래, 응용, 서비스, 서비스, 품질, 형태, 요구, 자원, 측면, 다양, 예상,...</td>\n",
       "      <td>미래의 응용서비스는 서비스 품질/형태 및 요구자원의 측면에서 초다양화가 예상된다. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129097</th>\n",
       "      <td>[미세, 구조, 제어, 기술, 비교, 분석, 압, 전, 소재, 입자, 형상, 입자,...</td>\n",
       "      <td>○ 미세구조 제어기술 비교분석(NKN계 압전소재) ○ BiTe계 입자형상 및 입자성...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42835</th>\n",
       "      <td>[기술, 슈퍼, 컴, 활용, 인체, 척추, 정보, 데이터베이스, 정밀, 모델, 가상...</td>\n",
       "      <td>IT 기술/슈퍼컴을 활용하여 인체 척추 정보 데이터베이스, 3D 고정밀 모델과 가상...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56045</th>\n",
       "      <td>[생체, 합성, 소재, 적용, 최적, 설계, 기법, 활용, 골, 내, 안정, 개선,...</td>\n",
       "      <td>- 생체적합성이 우수한 신소재(Ti-6Al-7Nb)를 적용하고, 최적설계 기법을 활...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42695</th>\n",
       "      <td>[클라우드, 컴퓨팅, 환경, 향상, 단말, 기반, 기술, 사용, 추상, 네트워크, ...</td>\n",
       "      <td>클라우드 컴퓨팅 환경에서 Security 및 Privacy를 향상시키기 위해 단말에...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137996</th>\n",
       "      <td>[기술, 이용, 단일, 세포, 내, 표적, 대상, 민감, 발, 현량, 차이, 공간,...</td>\n",
       "      <td>Bio-AFM 기술을 이용하여 (1) 단일세포 내 표적 miRNA를 대상으로 매우 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30832 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            for tokenizer  \\\n",
       "73688   [유전, 자료, 기반, 혈통, 재구, 보정, 기법, 확립, 고밀, 칩, 기반, 유전...   \n",
       "52162   [회전자, 내경, 가공, 장치, 개발, 정확, 위치, 프레임, 부품, 고정, 개발,...   \n",
       "159853  [연구, 공격, 취약점, 악용, 장치, 오작동, 탐지, 가상, 물리, 시스템, 내재...   \n",
       "113271  [주인, 집, 경우, 집, 안, 반려, 견, 스트레스, 정서, 안정, 예방, 외부,...   \n",
       "129806  [미래, 응용, 서비스, 서비스, 품질, 형태, 요구, 자원, 측면, 다양, 예상,...   \n",
       "...                                                   ...   \n",
       "129097  [미세, 구조, 제어, 기술, 비교, 분석, 압, 전, 소재, 입자, 형상, 입자,...   \n",
       "42835   [기술, 슈퍼, 컴, 활용, 인체, 척추, 정보, 데이터베이스, 정밀, 모델, 가상...   \n",
       "56045   [생체, 합성, 소재, 적용, 최적, 설계, 기법, 활용, 골, 내, 안정, 개선,...   \n",
       "42695   [클라우드, 컴퓨팅, 환경, 향상, 단말, 기반, 기술, 사용, 추상, 네트워크, ...   \n",
       "137996  [기술, 이용, 단일, 세포, 내, 표적, 대상, 민감, 발, 현량, 차이, 공간,...   \n",
       "\n",
       "                                                       전체  \n",
       "73688   유전형 자료 기반 혈통 재구성 및 보정기법 확립\\n\\n고밀도칩 기반 유전체 혈연행렬...  \n",
       "52162   ？ 회전자의 내경 가공 장치 개발\\n\\n   - 정확한 위치에 프레임과 부품을 고정...  \n",
       "159853  본 연구에서는 알려진 또는 알려지지 않은 공격을 통해 취약점을 악용하는 IoT 장치...  \n",
       "113271  - 주인이 집을 비울 경우 집 안에 홀로 남겨진 반려견의 스트레스와 정서적 불안정 ...  \n",
       "129806  미래의 응용서비스는 서비스 품질/형태 및 요구자원의 측면에서 초다양화가 예상된다. ...  \n",
       "...                                                   ...  \n",
       "129097  ○ 미세구조 제어기술 비교분석(NKN계 압전소재) ○ BiTe계 입자형상 및 입자성...  \n",
       "42835   IT 기술/슈퍼컴을 활용하여 인체 척추 정보 데이터베이스, 3D 고정밀 모델과 가상...  \n",
       "56045   - 생체적합성이 우수한 신소재(Ti-6Al-7Nb)를 적용하고, 최적설계 기법을 활...  \n",
       "42695   클라우드 컴퓨팅 환경에서 Security 및 Privacy를 향상시키기 위해 단말에...  \n",
       "137996  Bio-AFM 기술을 이용하여 (1) 단일세포 내 표적 miRNA를 대상으로 매우 ...  \n",
       "\n",
       "[30832 rows x 2 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = train_han[['for tokenizer','전체']].append(val_han[['for tokenizer','전체']])\n",
    "new_data = new_data.append(test_han[['for tokenizer','전체']])\n",
    "new_data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = list(new_data['for tokenizer']) \n",
    "tokenizer = Tokenizer(num_words=max_words_dic, lower=False, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(texts) \n",
    "word_dic = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_documents = list(train_han.cleaned_data)\n",
    "tr_labels = pd.get_dummies(train_han.label, prefix='label')\n",
    "vl_documents = list(val_han.cleaned_data)\n",
    "vl_labels = pd.get_dummies(val_han.label, prefix='label')\n",
    "ts_documents = list(test_han.cleaned_data)\n",
    "ts_labels = pd.get_dummies(test_han.label, prefix='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data = np.zeros((len(tr_documents), max_sentences, max_sent_length), dtype='int32') \n",
    "vl_data = np.zeros((len(vl_documents), max_sentences, max_sent_length), dtype='int32')\n",
    "ts_data = np.zeros((len(ts_documents), max_sentences, max_sent_length), dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3678it [00:07, 600.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'내셔날\n",
      "'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4318it [00:08, 502.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'내셔날\n",
      "'\n",
      "'내셔날\n",
      "'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6167it [00:12, 511.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# 데이터 구성 : [문서, 문장, 단어]\n",
    "doc_tuple = [(tr_documents, tr_data),(vl_documents, vl_data),(ts_documents, ts_data)]\n",
    "\n",
    "\n",
    "    \n",
    "for i, sentences in tqdm(enumerate(ts_documents)): \n",
    "    for j, sentence in enumerate(sentences):\n",
    "        if j < max_sentences:\n",
    "            wordTokens = text_to_word_sequence(str(sentence))\n",
    "            k = 0 \n",
    "            for _, word in enumerate(wordTokens):\n",
    "                try:\n",
    "                    word = eval(word)\n",
    "                    if k < max_sent_length :\n",
    "                        if word not in tokenizer.word_index:\n",
    "                            continue\n",
    "                        if tokenizer.word_index[word] < max_sent_length:\n",
    "                            ts_data[i,j,k] = tokenizer.word_index[word]\n",
    "                            k+=1\n",
    "\n",
    "                #except SyntaxError:\n",
    "                except:\n",
    "                    print(word)\n",
    "                    if k < max_sent_length : \n",
    "                        if word not in tokenizer.word_index:\n",
    "                            continue\n",
    "                        if tokenizer.word_index[word] < max_sent_length:\n",
    "                            ts_data[i,j,k] = tokenizer.word_index[word]\n",
    "                            k+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(x, kernel):\n",
    "    return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "\n",
    "\n",
    "class AttentionWithContext(Layer):\n",
    "    def __init__(self, W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                W_constraint=None, u_constraint=None, b_constraint=None, bias=True, **kwargs):\n",
    "        \n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "        \n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "        \n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "        \n",
    "        self.bias = bias \n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = self.add_weight(shape=(input_shape[-1], input_shape[-1],),\n",
    "                                initializer=self.init, \n",
    "                                name='{}_W'.format(self.name),\n",
    "                                regularizer=self.W_regularizer,\n",
    "                                constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape=(input_shape[-1],),\n",
    "                                    initializer=self.init, \n",
    "                                    name='{}_b'.format(self.name),\n",
    "                                    regularizer=self.b_regularizer,\n",
    "                                    constraint=self.b_constraint)\n",
    "        self.u = self.add_weight(shape=(input_shape[-1],),\n",
    "                                initializer=self.init,\n",
    "                                name='{}_u'.format(self.name),\n",
    "                                regularizer=self.u_regularizer,\n",
    "                                constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "    \n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "        \n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "        \n",
    "        a = K.exp(ait) \n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True)+K.epsilon(), K.floatx())\n",
    "        \n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X = [tr_data, vl_data, ts_data]\n",
    "data_y = [tr_labels, vl_labels, ts_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(data_x, data_y, max_sentences, max_sent_length):\n",
    "    # 데이터\n",
    "    x_train, x_valid, x_test = data_X[0], data_X[1], data_X[2]\n",
    "    y_train, y_valid, y_test = data_y[0], data_y[1], data_y[2] \n",
    "\n",
    "    embedding_layer = Embedding(len(tokenizer.word_index)+1, embedding_dim,\n",
    "                                input_length=max_sent_length, trainable=True)\n",
    "    \n",
    "    \n",
    "    word_input = Input(shape=(max_sent_length,), dtype='int32')  \n",
    "    word_sequences = embedding_layer(word_input) \n",
    "    word_lstm = Bidirectional(GRU(100, return_sequences=True))(word_sequences)\n",
    "    word_dense = TimeDistributed(Dense(200))(word_lstm)\n",
    "     word_att = AttentionWithContext()(word_dense)\n",
    "    wordEncoder = Model(word_input, word_att)\n",
    "    \n",
    "    sent_input = Input(shape=(max_sentences, max_sent_length), dtype='int32')\n",
    "    sent_encoder = TimeDistributed(wordEncoder)(sent_input)\n",
    "    sent_lstm = Bidirectional(GRU(100, return_sequences=True))(sent_encoder)\n",
    "    sent_dense = TimeDistributed(Dense(200))(sent_lstm)\n",
    "    sent_att = AttentionWithContext()(sent_dense)\n",
    "    latent_representation_of_doc = Dense(2)(sent_att)\n",
    "    predictions = Dense(2, activation='softmax')(sent_att)\n",
    "    model = Model(sent_input, predictions)\n",
    "    #Encoder = Model(sent_input, latent_representation_of_doc)\n",
    "    \n",
    "    model.compile(loss ='categorical_crossentropy', \n",
    "                 optimizer = adam,\n",
    "                 metrics = ['acc'])\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
    "    history = model.fit(x_train, y_train, validation_data=(x_valid, y_valid),\n",
    "                       epochs=2, batch_size=50, callbacks=[es])\n",
    "    \n",
    "    loss, acc = model.evaluate(x_train, y_train, verbose=2)\n",
    "    print('Training Acc: {:.4f}'.format(acc)) \n",
    "    \n",
    "    loss, acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "    print('Testing Acc: {:.4f}'.format(acc))\n",
    "    \n",
    "    #y_pred = np.argmax(model.predict(x_test), axis=1) +1\n",
    "    #cm = confusion_matrix(y_test, y_pred, labels=None, sample_weight=None)\n",
    "    \n",
    "    return model, history\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 30, 100)]         0         \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 30, 200)           2468990   \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 30, 200)           181200    \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 30, 200)           40200     \n",
      "_________________________________________________________________\n",
      "attention_with_context_3 (At (None, 200)               40400     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 2,731,192\n",
      "Trainable params: 2,731,192\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "1186/1186 [==============================] - 5236s 4s/step - loss: 0.3805 - acc: 0.8203 - val_loss: 0.3629 - val_acc: 0.8302\n",
      "Epoch 2/3\n",
      "1186/1186 [==============================] - 5360s 5s/step - loss: 0.3580 - acc: 0.8318 - val_loss: 0.3518 - val_acc: 0.8338\n",
      "Epoch 3/3\n",
      "1186/1186 [==============================] - 5031s 4s/step - loss: 0.3512 - acc: 0.8340 - val_loss: 0.3488 - val_acc: 0.8316\n",
      "3705/3705 - 1524s - loss: 0.3451 - acc: 0.8364\n",
      "Training Acc: 0.8364\n",
      "1158/1158 - 477s - loss: 0.3497 - acc: 0.8314\n",
      "Testing Acc: 0.8314\n"
     ]
    }
   ],
   "source": [
    "model, history = run_model(data_X, data_y, max_sentences, max_sent_length) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_0</th>\n",
       "      <th>label_1</th>\n",
       "      <th>for tokenizer</th>\n",
       "      <th>전체</th>\n",
       "      <th>ICT_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1113</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[차년, 개발, 기반, 브로커, 개발, 구성, 요소, 간, 다양, 연동, 테스트, ...</td>\n",
       "      <td>1차년도에 개발된 Thin-Agent와 Connected Resource Manag...</td>\n",
       "      <td>0.955209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103341</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[당사, 푸시, 서비스, 개발, 확보, 다운스, 트림, 통신, 기술, 바탕, 업, ...</td>\n",
       "      <td>당사가 푸시 서비스를 개발하면서 확보한 다운스트림 통신 기술을 바탕으로 업스트림 통...</td>\n",
       "      <td>0.951943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145858</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[지능, 투어, 가이드, 시스템, 이하, 시스템, 투어, 가이드, 앱, 방문자, 실...</td>\n",
       "      <td>지능형 투어 가이드 시스템(이하 ITG(Intelligent Tour Guide)시...</td>\n",
       "      <td>0.943554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[정형, 정형, 형태, 데이터, 수집, 표준, 포맷, 변환, 고성능, 처리, 기능,...</td>\n",
       "      <td>ㅇ 정형, 비정형의 어떠한 형태의 데이터도 수집하여 표준화 된 포맷으로 변환하여 고...</td>\n",
       "      <td>0.942266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58304</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[기술, 개발, 최종, 목표, 선박, 승선, 여행, 여객, 선원, 육상, 가족, 연...</td>\n",
       "      <td>1. 기술개발 최종목표\\n\\n\\n\\n   선박을 승선하여 여행하는 여객이나, 선원,...</td>\n",
       "      <td>0.942209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14583</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[연구, 저온, 신호, 인식, 전달, 기능, 사이토, 키닌, 중, 사이토, 키닌, ...</td>\n",
       "      <td>본 연구는 저온 신호의 인식과 전달 기능을 가지고 있는 사이토키닌 twocompon...</td>\n",
       "      <td>0.000580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136750</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[신장, 질환, 핵, 수용체, 이용, 신장, 번역, 후, 변형, 기능, 신규, 표,...</td>\n",
       "      <td>신장질환에서 핵 수용체 FXR을 이용하여 &lt;신장 FXR의 번역 후 변형 기능과 신규...</td>\n",
       "      <td>0.000576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25130</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[유전자, 발현, 조절, 기능, 발현, 조절, 기능, 분리, 인식, 이, 단계, 관...</td>\n",
       "      <td>유전자 발현 조절에 기능하는 STA1-mediated RNA regulatory n...</td>\n",
       "      <td>0.000523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162509</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[연구, 개, 유전자, 포함, 애기장대, 단백질, 유전자, 중, 개, 유전자, 대상...</td>\n",
       "      <td>본 연구에서는 42개 유전자를 포함하는 애기장대 non-tandem CCCH-typ...</td>\n",
       "      <td>0.000511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47386</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[꼬마, 선충, 유전자, 수명, 조절, 연구, 꼬마, 선충, 유전자, 수명, 조절,...</td>\n",
       "      <td>예쁜꼬마선충의 PDZ domain 유전자에 의한 수명 조절 연구 예쁜꼬마선충의 PD...</td>\n",
       "      <td>0.000470</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37050 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        label_0  label_1                                      for tokenizer  \\\n",
       "1113          0        1  [차년, 개발, 기반, 브로커, 개발, 구성, 요소, 간, 다양, 연동, 테스트, ...   \n",
       "103341        0        1  [당사, 푸시, 서비스, 개발, 확보, 다운스, 트림, 통신, 기술, 바탕, 업, ...   \n",
       "145858        0        1  [지능, 투어, 가이드, 시스템, 이하, 시스템, 투어, 가이드, 앱, 방문자, 실...   \n",
       "1168          0        1  [정형, 정형, 형태, 데이터, 수집, 표준, 포맷, 변환, 고성능, 처리, 기능,...   \n",
       "58304         0        1  [기술, 개발, 최종, 목표, 선박, 승선, 여행, 여객, 선원, 육상, 가족, 연...   \n",
       "...         ...      ...                                                ...   \n",
       "14583         1        0  [연구, 저온, 신호, 인식, 전달, 기능, 사이토, 키닌, 중, 사이토, 키닌, ...   \n",
       "136750        1        0  [신장, 질환, 핵, 수용체, 이용, 신장, 번역, 후, 변형, 기능, 신규, 표,...   \n",
       "25130         1        0  [유전자, 발현, 조절, 기능, 발현, 조절, 기능, 분리, 인식, 이, 단계, 관...   \n",
       "162509        1        0  [연구, 개, 유전자, 포함, 애기장대, 단백질, 유전자, 중, 개, 유전자, 대상...   \n",
       "47386         1        0  [꼬마, 선충, 유전자, 수명, 조절, 연구, 꼬마, 선충, 유전자, 수명, 조절,...   \n",
       "\n",
       "                                                       전체  ICT_prob  \n",
       "1113    1차년도에 개발된 Thin-Agent와 Connected Resource Manag...  0.955209  \n",
       "103341  당사가 푸시 서비스를 개발하면서 확보한 다운스트림 통신 기술을 바탕으로 업스트림 통...  0.951943  \n",
       "145858  지능형 투어 가이드 시스템(이하 ITG(Intelligent Tour Guide)시...  0.943554  \n",
       "1168    ㅇ 정형, 비정형의 어떠한 형태의 데이터도 수집하여 표준화 된 포맷으로 변환하여 고...  0.942266  \n",
       "58304   1. 기술개발 최종목표\\n\\n\\n\\n   선박을 승선하여 여행하는 여객이나, 선원,...  0.942209  \n",
       "...                                                   ...       ...  \n",
       "14583   본 연구는 저온 신호의 인식과 전달 기능을 가지고 있는 사이토키닌 twocompon...  0.000580  \n",
       "136750  신장질환에서 핵 수용체 FXR을 이용하여 <신장 FXR의 번역 후 변형 기능과 신규...  0.000576  \n",
       "25130   유전자 발현 조절에 기능하는 STA1-mediated RNA regulatory n...  0.000523  \n",
       "162509  본 연구에서는 42개 유전자를 포함하는 애기장대 non-tandem CCCH-typ...  0.000511  \n",
       "47386   예쁜꼬마선충의 PDZ domain 유전자에 의한 수명 조절 연구 예쁜꼬마선충의 PD...  0.000470  \n",
       "\n",
       "[37050 rows x 5 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ict_prob = model.predict(ts_data) \n",
    "han_df = pd.DataFrame(ts_labels)\n",
    "han_df = pd.merge(han_df, new_data, left_index=True, right_index=True, how='left')\n",
    "han_df['ICT_prob'] = ict_prob[:,1]\n",
    "han_df = han_df.sort_values(by='ICT_prob', ascending=False)\n",
    "y_pred= [1 if prob >= 0.5 else 0 for prob in han_df.ICT_prob]\n",
    "han_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "han_df[['전체','label_1','ICT_prob']].to_csv('./data/ntis_classification_by_han_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>전체</th>\n",
       "      <th>label_1</th>\n",
       "      <th>ICT_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>135888</th>\n",
       "      <td>4차 산업 혁명시대의 사물 인터넷 인프라에 호환이 되며 최적의 적합성을 가진 스마트...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161870</th>\n",
       "      <td>- 각종 도로 및 철도 차량 대상으로 5G 기반 통신 서비스를 제공하기 위한 Veh...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33307</th>\n",
       "      <td>클라우드 컴퓨팅은 유비쿼터스 환경으로 급변하는 현재 지식서비스 분야에서 가장 유망한...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57743</th>\n",
       "      <td>？ 본 사업은 농업 IT 융합 기술인 도시형 인삼, 약용 식물 공장을 위한 센서네트...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>제안 기술의 개요 및 개발 방향 \\n\\n\\n\\n- 퀄컴사의 오픈 플랫폼인 Allj...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124737</th>\n",
       "      <td>일산화질소 (nitric oxide, NO)는 생체 내에서 생리활성물질로서 다양한 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.054581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182797</th>\n",
       "      <td>최종 목표: 구강암 발생 마우스 모델 이용 G 단백질 연결 수용체 54(G prot...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.051975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182555</th>\n",
       "      <td>조기 재활치료와 뇌졸중 환자의 기능 회복이 보편적으로 양의 상관관계가 있다고 생각되...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.051806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22062</th>\n",
       "      <td>연구내용\\n   1. EST data base를 이용한 새로운 isoform의 xe...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.048991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124394</th>\n",
       "      <td>우리나라에서 위암은 단위인구당 위암 발생률이 전세계적으로 가장 높을 정도로 호발하는...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.046395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6167 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       전체  label_1  ICT_prob\n",
       "135888  4차 산업 혁명시대의 사물 인터넷 인프라에 호환이 되며 최적의 적합성을 가진 스마트...        1  0.999179\n",
       "161870  - 각종 도로 및 철도 차량 대상으로 5G 기반 통신 서비스를 제공하기 위한 Veh...        1  0.998625\n",
       "33307   클라우드 컴퓨팅은 유비쿼터스 환경으로 급변하는 현재 지식서비스 분야에서 가장 유망한...        1  0.998605\n",
       "57743   ？ 본 사업은 농업 IT 융합 기술인 도시형 인삼, 약용 식물 공장을 위한 센서네트...        1  0.998557\n",
       "1026     제안 기술의 개요 및 개발 방향 \\n\\n\\n\\n- 퀄컴사의 오픈 플랫폼인 Allj...        1  0.998459\n",
       "...                                                   ...      ...       ...\n",
       "124737  일산화질소 (nitric oxide, NO)는 생체 내에서 생리활성물질로서 다양한 ...        0  0.054581\n",
       "182797  최종 목표: 구강암 발생 마우스 모델 이용 G 단백질 연결 수용체 54(G prot...        0  0.051975\n",
       "182555  조기 재활치료와 뇌졸중 환자의 기능 회복이 보편적으로 양의 상관관계가 있다고 생각되...        0  0.051806\n",
       "22062   연구내용\\n   1. EST data base를 이용한 새로운 isoform의 xe...        0  0.048991\n",
       "124394  우리나라에서 위암은 단위인구당 위암 발생률이 전세계적으로 가장 높을 정도로 호발하는...        0  0.046395\n",
       "\n",
       "[6167 rows x 3 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "han_df = pd.read_csv('./my_data/han/ntis_classification_by_han_Data.csv', index_col=0)\n",
    "han_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.92      0.90     29122\n",
      "           1       0.63      0.51      0.57      7928\n",
      "\n",
      "    accuracy                           0.83     37050\n",
      "   macro avg       0.75      0.72      0.73     37050\n",
      "weighted avg       0.82      0.83      0.82     37050\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(han_df.label_1, y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "fpr, tpr, thresholds = roc_curve(lda_df.label_1, y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--') \n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC_curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "fpr, tpr, thresholds = roc_curve(latent_representation.label_1, y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC_curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 뉴스  데이터 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.read_csv('./data/최종본_KBS_2017.csv')\n",
    "news_df = news_df.drop_duplicates(['content'])\n",
    "news = news_df[['content']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text_list):\n",
    "    preprocessed_text = []\n",
    "    for text in text_list:\n",
    "        text = re.sub(r'\\W+', ' ', text)\n",
    "        text = re.sub('o','', text)\n",
    "        text = re.sub('○','', text)\n",
    "        text = re.sub('ㅇ','', text)\n",
    "        text = mecab.nouns(text)\n",
    "        preprocessed_text.append(text)\n",
    "    return preprocessed_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kiki/miniconda3/lib/python3.8/site-packages/pandas/core/generic.py:5303: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[앵커, 트, 치열, 경쟁, 미래, 앞, 우리, 청년, 삶], [이, 건, 긍정,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[앵커, 트, 이번, 곳, 철강, 생산, 현장, 연결], [포항, 제철소, 김수영...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[앵커, 트, 새해, 첫날, 해, 기원, 소망, 다짐, 가슴, 분], [서울, 광...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[앵커, 트, 지금, 우리, 경제, 주력, 산업, 도약, 현장, 이번, 앞, 우리...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[앵커, 트, 최문종, 오프닝, 공, 웅조, 이곳, 서울, 명동], [시간, 가족...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7493</th>\n",
       "      <td>[[년, 마지막], [이제, 올해, 시간], [새해, 첫날, 내일, 해돋이, 계획,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7494</th>\n",
       "      <td>[[앵커, 트, 올해, 마지막, 날, 오늘, 광주, 아파트, 불, 나, 아이, 명,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7495</th>\n",
       "      <td>[[올해, 마지막, 날], [밤, 사이, 눈, 비, 미세먼지, 농도, 대기], [수...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7496</th>\n",
       "      <td>[[앵커, 트, 올해, 마지막, 날, 광주, 아파트, 불, 나, 아이, 명, 사고]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7497</th>\n",
       "      <td>[[앵커, 트, 사건, 사고, 올해, 위기, 현장, 타인, 목숨, 우리, 주변, 영...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6229 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                content\n",
       "0     [[앵커, 트, 치열, 경쟁, 미래, 앞, 우리, 청년, 삶], [이, 건, 긍정,...\n",
       "1     [[앵커, 트, 이번, 곳, 철강, 생산, 현장, 연결], [포항, 제철소, 김수영...\n",
       "2     [[앵커, 트, 새해, 첫날, 해, 기원, 소망, 다짐, 가슴, 분], [서울, 광...\n",
       "3     [[앵커, 트, 지금, 우리, 경제, 주력, 산업, 도약, 현장, 이번, 앞, 우리...\n",
       "4     [[앵커, 트, 최문종, 오프닝, 공, 웅조, 이곳, 서울, 명동], [시간, 가족...\n",
       "...                                                 ...\n",
       "7493  [[년, 마지막], [이제, 올해, 시간], [새해, 첫날, 내일, 해돋이, 계획,...\n",
       "7494  [[앵커, 트, 올해, 마지막, 날, 오늘, 광주, 아파트, 불, 나, 아이, 명,...\n",
       "7495  [[올해, 마지막, 날], [밤, 사이, 눈, 비, 미세먼지, 농도, 대기], [수...\n",
       "7496  [[앵커, 트, 올해, 마지막, 날, 광주, 아파트, 불, 나, 아이, 명, 사고]...\n",
       "7497  [[앵커, 트, 사건, 사고, 올해, 위기, 현장, 타인, 목숨, 우리, 주변, 영...\n",
       "\n",
       "[6229 rows x 1 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.content = news.content.apply(lambda text: nltk.sent_tokenize(text))\n",
    "news.content = news.content.apply(preprocessing) \n",
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6229it [00:05, 1135.89it/s]\n"
     ]
    }
   ],
   "source": [
    "data = np.zeros((len(news), max_sentences, max_sent_length), dtype='int32') \n",
    "documents = list(news.content)\n",
    "\n",
    "\n",
    "for i, sentences in tqdm(enumerate(documents)):\n",
    "    for j, sentence in enumerate(sentences): \n",
    "        if j < max_sentences:\n",
    "            wordTokens = text_to_word_sequence(str(sentence))\n",
    "            k = 0 \n",
    "            for _, word in enumerate(wordTokens):\n",
    "                try:\n",
    "                    word = eval(word)\n",
    "                    if k < max_sent_length :\n",
    "                        if word not in tokenizer.word_index:\n",
    "                            continue\n",
    "                        if tokenizer.word_index[word] < max_sent_length:\n",
    "                            data[i,j,k] = tokenizer.word_index[word]\n",
    "                            k+=1\n",
    "                    \n",
    "                except SyntaxError:\n",
    "                    print(word)\n",
    "                    if k < max_sent_length :\n",
    "                        if word not in tokenizer.word_index:\n",
    "                            continue\n",
    "                        if tokenizer.word_index[word] < max_sent_length:\n",
    "                            data[i,j,k] = tokenizer.word_index[word]\n",
    "                            k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-95-956ef803a3bf>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  news['ICT_prob'] = Ict_prob[:,1]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>ICT_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[앵커, 트, 치열, 경쟁, 미래, 앞, 우리, 청년, 삶], [이, 건, 긍정,...</td>\n",
       "      <td>0.136031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[앵커, 트, 이번, 곳, 철강, 생산, 현장, 연결], [포항, 제철소, 김수영...</td>\n",
       "      <td>0.087283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[앵커, 트, 새해, 첫날, 해, 기원, 소망, 다짐, 가슴, 분], [서울, 광...</td>\n",
       "      <td>0.010626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[앵커, 트, 지금, 우리, 경제, 주력, 산업, 도약, 현장, 이번, 앞, 우리...</td>\n",
       "      <td>0.169594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[앵커, 트, 최문종, 오프닝, 공, 웅조, 이곳, 서울, 명동], [시간, 가족...</td>\n",
       "      <td>0.007267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7493</th>\n",
       "      <td>[[년, 마지막], [이제, 올해, 시간], [새해, 첫날, 내일, 해돋이, 계획,...</td>\n",
       "      <td>0.076800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7494</th>\n",
       "      <td>[[앵커, 트, 올해, 마지막, 날, 오늘, 광주, 아파트, 불, 나, 아이, 명,...</td>\n",
       "      <td>0.359094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7495</th>\n",
       "      <td>[[올해, 마지막, 날], [밤, 사이, 눈, 비, 미세먼지, 농도, 대기], [수...</td>\n",
       "      <td>0.054395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7496</th>\n",
       "      <td>[[앵커, 트, 올해, 마지막, 날, 광주, 아파트, 불, 나, 아이, 명, 사고]...</td>\n",
       "      <td>0.280817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7497</th>\n",
       "      <td>[[앵커, 트, 사건, 사고, 올해, 위기, 현장, 타인, 목숨, 우리, 주변, 영...</td>\n",
       "      <td>0.046833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6229 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                content  ICT_prob\n",
       "0     [[앵커, 트, 치열, 경쟁, 미래, 앞, 우리, 청년, 삶], [이, 건, 긍정,...  0.136031\n",
       "1     [[앵커, 트, 이번, 곳, 철강, 생산, 현장, 연결], [포항, 제철소, 김수영...  0.087283\n",
       "2     [[앵커, 트, 새해, 첫날, 해, 기원, 소망, 다짐, 가슴, 분], [서울, 광...  0.010626\n",
       "3     [[앵커, 트, 지금, 우리, 경제, 주력, 산업, 도약, 현장, 이번, 앞, 우리...  0.169594\n",
       "4     [[앵커, 트, 최문종, 오프닝, 공, 웅조, 이곳, 서울, 명동], [시간, 가족...  0.007267\n",
       "...                                                 ...       ...\n",
       "7493  [[년, 마지막], [이제, 올해, 시간], [새해, 첫날, 내일, 해돋이, 계획,...  0.076800\n",
       "7494  [[앵커, 트, 올해, 마지막, 날, 오늘, 광주, 아파트, 불, 나, 아이, 명,...  0.359094\n",
       "7495  [[올해, 마지막, 날], [밤, 사이, 눈, 비, 미세먼지, 농도, 대기], [수...  0.054395\n",
       "7496  [[앵커, 트, 올해, 마지막, 날, 광주, 아파트, 불, 나, 아이, 명, 사고]...  0.280817\n",
       "7497  [[앵커, 트, 사건, 사고, 올해, 위기, 현장, 타인, 목숨, 우리, 주변, 영...  0.046833\n",
       "\n",
       "[6229 rows x 2 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ict_prob = model.predict(data)\n",
    "news['ICT_prob'] = Ict_prob[:,1]\n",
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-96-fcbf8ba1d78d>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  news['original_news'] = news_df.content\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>ICT_prob</th>\n",
       "      <th>original_news</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[앵커, 트, 치열, 경쟁, 미래, 앞, 우리, 청년, 삶], [이, 건, 긍정,...</td>\n",
       "      <td>0.136031</td>\n",
       "      <td>&lt;앵커 멘트&gt; 치열한 경쟁과 불확실한 미래 앞에 서 있는 우리 청년들의 삶은 만만치...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[앵커, 트, 이번, 곳, 철강, 생산, 현장, 연결], [포항, 제철소, 김수영...</td>\n",
       "      <td>0.087283</td>\n",
       "      <td>&lt;앵커 멘트&gt; 이번엔 어느 곳보다 뜨거운 철강 생산 현장을 연결합니다. 포항제철소에...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[앵커, 트, 새해, 첫날, 해, 기원, 소망, 다짐, 가슴, 분], [서울, 광...</td>\n",
       "      <td>0.010626</td>\n",
       "      <td>&lt;앵커 멘트&gt; 새해 첫날, 보다 나은 한 해를 기원하며 소망과 다짐을 가슴에 새긴 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[앵커, 트, 지금, 우리, 경제, 주력, 산업, 도약, 현장, 이번, 앞, 우리...</td>\n",
       "      <td>0.169594</td>\n",
       "      <td>&lt;앵커 멘트&gt; 지금까지 우리 경제를 이끌어왔던 주력 산업의 재도약 현장들을 보셨는데...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[앵커, 트, 최문종, 오프닝, 공, 웅조, 이곳, 서울, 명동], [시간, 가족...</td>\n",
       "      <td>0.007267</td>\n",
       "      <td>&lt;앵커 멘트&gt; &lt;최문종오프닝+공웅조&gt; 네, 이곳은 서울 명동입니다. 늦은 시간인데도...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7493</th>\n",
       "      <td>[[년, 마지막], [이제, 올해, 시간], [새해, 첫날, 내일, 해돋이, 계획,...</td>\n",
       "      <td>0.076800</td>\n",
       "      <td>2017년의 마지막 해도 저물었습니다. 이제는 올해가 5시간도 채 남지 않았는데요....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7494</th>\n",
       "      <td>[[앵커, 트, 올해, 마지막, 날, 오늘, 광주, 아파트, 불, 나, 아이, 명,...</td>\n",
       "      <td>0.359094</td>\n",
       "      <td>&lt;앵커 멘트&gt; 올해 마지막 날인 오늘 광주의 한 아파트에 불이 나 어린 아이 세 명...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7495</th>\n",
       "      <td>[[올해, 마지막, 날], [밤, 사이, 눈, 비, 미세먼지, 농도, 대기], [수...</td>\n",
       "      <td>0.054395</td>\n",
       "      <td>어느덧 올해 마지막 날입니다. 밤사이 내린 눈,비로 미세먼지 농도가 옅어지긴 했지만...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7496</th>\n",
       "      <td>[[앵커, 트, 올해, 마지막, 날, 광주, 아파트, 불, 나, 아이, 명, 사고]...</td>\n",
       "      <td>0.280817</td>\n",
       "      <td>&lt;앵커 멘트&gt; 올해 마지막 날 광주의 한 아파트에 불이 나 어린 아이 세 명이 숨지...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7497</th>\n",
       "      <td>[[앵커, 트, 사건, 사고, 올해, 위기, 현장, 타인, 목숨, 우리, 주변, 영...</td>\n",
       "      <td>0.046833</td>\n",
       "      <td>&lt;앵커 멘트&gt; 이런 저런 사건사고가 끊이지 않았던 올해 였지만, 위기의 현장에서 타...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6229 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                content  ICT_prob  \\\n",
       "0     [[앵커, 트, 치열, 경쟁, 미래, 앞, 우리, 청년, 삶], [이, 건, 긍정,...  0.136031   \n",
       "1     [[앵커, 트, 이번, 곳, 철강, 생산, 현장, 연결], [포항, 제철소, 김수영...  0.087283   \n",
       "2     [[앵커, 트, 새해, 첫날, 해, 기원, 소망, 다짐, 가슴, 분], [서울, 광...  0.010626   \n",
       "3     [[앵커, 트, 지금, 우리, 경제, 주력, 산업, 도약, 현장, 이번, 앞, 우리...  0.169594   \n",
       "4     [[앵커, 트, 최문종, 오프닝, 공, 웅조, 이곳, 서울, 명동], [시간, 가족...  0.007267   \n",
       "...                                                 ...       ...   \n",
       "7493  [[년, 마지막], [이제, 올해, 시간], [새해, 첫날, 내일, 해돋이, 계획,...  0.076800   \n",
       "7494  [[앵커, 트, 올해, 마지막, 날, 오늘, 광주, 아파트, 불, 나, 아이, 명,...  0.359094   \n",
       "7495  [[올해, 마지막, 날], [밤, 사이, 눈, 비, 미세먼지, 농도, 대기], [수...  0.054395   \n",
       "7496  [[앵커, 트, 올해, 마지막, 날, 광주, 아파트, 불, 나, 아이, 명, 사고]...  0.280817   \n",
       "7497  [[앵커, 트, 사건, 사고, 올해, 위기, 현장, 타인, 목숨, 우리, 주변, 영...  0.046833   \n",
       "\n",
       "                                          original_news  \n",
       "0     <앵커 멘트> 치열한 경쟁과 불확실한 미래 앞에 서 있는 우리 청년들의 삶은 만만치...  \n",
       "1     <앵커 멘트> 이번엔 어느 곳보다 뜨거운 철강 생산 현장을 연결합니다. 포항제철소에...  \n",
       "2     <앵커 멘트> 새해 첫날, 보다 나은 한 해를 기원하며 소망과 다짐을 가슴에 새긴 ...  \n",
       "3     <앵커 멘트> 지금까지 우리 경제를 이끌어왔던 주력 산업의 재도약 현장들을 보셨는데...  \n",
       "4     <앵커 멘트> <최문종오프닝+공웅조> 네, 이곳은 서울 명동입니다. 늦은 시간인데도...  \n",
       "...                                                 ...  \n",
       "7493  2017년의 마지막 해도 저물었습니다. 이제는 올해가 5시간도 채 남지 않았는데요....  \n",
       "7494  <앵커 멘트> 올해 마지막 날인 오늘 광주의 한 아파트에 불이 나 어린 아이 세 명...  \n",
       "7495  어느덧 올해 마지막 날입니다. 밤사이 내린 눈,비로 미세먼지 농도가 옅어지긴 했지만...  \n",
       "7496  <앵커 멘트> 올해 마지막 날 광주의 한 아파트에 불이 나 어린 아이 세 명이 숨지...  \n",
       "7497  <앵커 멘트> 이런 저런 사건사고가 끊이지 않았던 올해 였지만, 위기의 현장에서 타...  \n",
       "\n",
       "[6229 rows x 3 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news['original_news'] = news_df.content\n",
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = news.sort_values(by='ICT_prob',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>ICT_prob</th>\n",
       "      <th>original_news</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2551</th>\n",
       "      <td>[[앵커, 트, 부모, 독립, 결혼, 여기, 고령, 영향, 인, 가구], [앞, 년...</td>\n",
       "      <td>0.845094</td>\n",
       "      <td>&lt;앵커 멘트&gt; 부모로부터 독립해 살고, 결혼이 늦어지고 여기에 고령화의 영향까지 겹...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>[[앵커, 트, 뷰티], [한국, 화장품, 한류, 영향, 아시아, 시장, 석권, 여...</td>\n",
       "      <td>0.821510</td>\n",
       "      <td>&lt;앵커 멘트&gt; 'K-뷰티'라고 들어보셨지요. 한국 화장품이 한류 영향으로 아시아 시...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>[[앵커, 트, 미국, 보호, 무역, 주, 강화, 중국, 사드, 보복, 조치, 등,...</td>\n",
       "      <td>0.782220</td>\n",
       "      <td>&lt;앵커 멘트&gt; 미국의 보호무역주의 강화와 중국의 사드 보복 조치 등으로 수출 환경이...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7092</th>\n",
       "      <td>[[앵커, 트, 우리, 국민, 명, 중, 명, 암유, 병자, 것], [년, 기준, ...</td>\n",
       "      <td>0.779282</td>\n",
       "      <td>&lt;앵커 멘트&gt; 우리 국민 31명 중 1명은 '암유병자'인 것으로 나타났습니다. 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6781</th>\n",
       "      <td>[[앵커, 트, 정부, 임대, 주택, 등록, 활성, 임대, 주택, 사업자, 등록, ...</td>\n",
       "      <td>0.773388</td>\n",
       "      <td>&lt;앵커 멘트&gt; 정부가 임대주택 등록을 활성화하기 위해 임대주택 사업자로 등록할 경우...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2064</th>\n",
       "      <td>[[앵커, 트, 서울, 병원, 남자, 간호사, 휴대, 전화, 동료, 여성, 간호사,...</td>\n",
       "      <td>0.001003</td>\n",
       "      <td>&lt;앵커 멘트&gt; 서울의 한 병원에서 남자 간호사가 휴대전화로 동료 여성 간호사들의 신...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2069</th>\n",
       "      <td>[[앵커, 트, 병원, 남자, 간호사, 휴대, 전화, 여자, 간호사, 신체, 적발]...</td>\n",
       "      <td>0.001003</td>\n",
       "      <td>&lt;앵커 멘트&gt; 한 병원에서 남자 간호사가 휴대전화로 여자 간호사들의 신체를 몰래 찍...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3804</th>\n",
       "      <td>[[앵커, 트, 십, 대, 또래, 학생, 구타, 영상, 촬영, 유포, 일, 우리나라...</td>\n",
       "      <td>0.000983</td>\n",
       "      <td>&lt;앵커 멘트&gt; 십 대들이 또래 학생을 구타하고 영상을 촬영해 유포하는 일, 우리나라...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5139</th>\n",
       "      <td>[[앵커, 트, 부산, 경찰청, 불법, 몰래카메라, 경고, 영상, 몰카, 유통, 사...</td>\n",
       "      <td>0.000958</td>\n",
       "      <td>&lt;앵커 멘트&gt; 부산경찰청이 불법 몰래카메라를 가장한 경고 영상을 만들어 몰카가 유통...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6291</th>\n",
       "      <td>[[앵커, 트, 미국, 플로리다, 농장, 농부, 소, 집단, 발, 학대, 영상, 공...</td>\n",
       "      <td>0.000953</td>\n",
       "      <td>&lt;앵커 멘트&gt; 미국 플로리다의 한 농장에서 농부들이 소를 집단으로 때리고 발로 차며...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6229 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                content  ICT_prob  \\\n",
       "2551  [[앵커, 트, 부모, 독립, 결혼, 여기, 고령, 영향, 인, 가구], [앞, 년...  0.845094   \n",
       "731   [[앵커, 트, 뷰티], [한국, 화장품, 한류, 영향, 아시아, 시장, 석권, 여...  0.821510   \n",
       "971   [[앵커, 트, 미국, 보호, 무역, 주, 강화, 중국, 사드, 보복, 조치, 등,...  0.782220   \n",
       "7092  [[앵커, 트, 우리, 국민, 명, 중, 명, 암유, 병자, 것], [년, 기준, ...  0.779282   \n",
       "6781  [[앵커, 트, 정부, 임대, 주택, 등록, 활성, 임대, 주택, 사업자, 등록, ...  0.773388   \n",
       "...                                                 ...       ...   \n",
       "2064  [[앵커, 트, 서울, 병원, 남자, 간호사, 휴대, 전화, 동료, 여성, 간호사,...  0.001003   \n",
       "2069  [[앵커, 트, 병원, 남자, 간호사, 휴대, 전화, 여자, 간호사, 신체, 적발]...  0.001003   \n",
       "3804  [[앵커, 트, 십, 대, 또래, 학생, 구타, 영상, 촬영, 유포, 일, 우리나라...  0.000983   \n",
       "5139  [[앵커, 트, 부산, 경찰청, 불법, 몰래카메라, 경고, 영상, 몰카, 유통, 사...  0.000958   \n",
       "6291  [[앵커, 트, 미국, 플로리다, 농장, 농부, 소, 집단, 발, 학대, 영상, 공...  0.000953   \n",
       "\n",
       "                                          original_news  \n",
       "2551  <앵커 멘트> 부모로부터 독립해 살고, 결혼이 늦어지고 여기에 고령화의 영향까지 겹...  \n",
       "731   <앵커 멘트> 'K-뷰티'라고 들어보셨지요. 한국 화장품이 한류 영향으로 아시아 시...  \n",
       "971   <앵커 멘트> 미국의 보호무역주의 강화와 중국의 사드 보복 조치 등으로 수출 환경이...  \n",
       "7092  <앵커 멘트> 우리 국민 31명 중 1명은 '암유병자'인 것으로 나타났습니다. 20...  \n",
       "6781  <앵커 멘트> 정부가 임대주택 등록을 활성화하기 위해 임대주택 사업자로 등록할 경우...  \n",
       "...                                                 ...  \n",
       "2064  <앵커 멘트> 서울의 한 병원에서 남자 간호사가 휴대전화로 동료 여성 간호사들의 신...  \n",
       "2069  <앵커 멘트> 한 병원에서 남자 간호사가 휴대전화로 여자 간호사들의 신체를 몰래 찍...  \n",
       "3804  <앵커 멘트> 십 대들이 또래 학생을 구타하고 영상을 촬영해 유포하는 일, 우리나라...  \n",
       "5139  <앵커 멘트> 부산경찰청이 불법 몰래카메라를 가장한 경고 영상을 만들어 몰카가 유통...  \n",
       "6291  <앵커 멘트> 미국 플로리다의 한 농장에서 농부들이 소를 집단으로 때리고 발로 차며...  \n",
       "\n",
       "[6229 rows x 3 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news[['original_news','ICT_prob']].to_csv('./data/news_classification_by_han_Data.csv', header=True, encoding='utf-8-sig',index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
